{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59208315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import uuid\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import StructType, StringType, DoubleType, TimestampType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "PROJECT_PATH = Path.cwd().parent\n",
    "DATA_DIR = '.data'\n",
    "DATA_PATH = PROJECT_PATH / DATA_DIR\n",
    "DATA_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "output_folder = str(DATA_PATH)\n",
    "checkpoint_path = str(Path.cwd() / 'checkpoint')\n",
    "\n",
    "file_schema = StructType() \\\n",
    "    .add('id', StringType()) \\\n",
    "    .add('temperature', DoubleType()) \\\n",
    "    .add('timestamp', TimestampType())\n",
    "\n",
    "schema_name = 'dp700_e011'\n",
    "table_name = 'temperature_stream'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0459b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/21 14:49:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('test').master('local[*]').getOrCreate()\n",
    "spark.sql(f'CREATE SCHEMA IF NOT EXISTS {schema_name}')\n",
    "table_location = f'Tables/{schema_name}/{table_name}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75412596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/21 14:49:08 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "raw_stream_df = spark.readStream \\\n",
    "    .schema(file_schema) \\\n",
    "    .option('maxFilesPerTrigger', 1) \\\n",
    "    .json(output_folder)\n",
    "\n",
    "transformed_stream_df = raw_stream_df \\\n",
    "    .withColumn('processed_timestamp',\n",
    "        F.current_timestamp())\n",
    "\n",
    "delta_stream = transformed_stream_df.writeStream \\\n",
    "    .format('parquet') \\\n",
    "    .outputMode('append') \\\n",
    "    .option('checkpointLocation', checkpoint_path) \\\n",
    "    .start(f'Tables/{schema_name}/{table_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c67c4920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.range(1).select(F.current_schema()).show()\n",
    "# spark.range(1).select(F.current_catalog()).show()\n",
    "# for stream in spark.streams.active:\n",
    "#     print(f\"ID: {stream.id}, Name: {stream.name}, Active: {stream.isActive}\")\n",
    "# # Ensure you are in the correct schema\n",
    "# spark.sql(f\"USE {schema_name}\")\n",
    "\n",
    "# # See what tables are available\n",
    "# print(f\"Tables in schema '{schema_name}':\")\n",
    "# spark.sql(\"SHOW TABLES\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "854501a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting 15 seconds for the first micro-batch to complete...\n",
      "Creating table 'dp700_e011.temperature_stream'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/21 14:49:23 WARN HadoopFSUtils: The directory file:/workspaces/aleksei-partanen-spark-structured-streaming/nbs/spark-warehouse/dp700_e011.db/Tables/dp700_e011/temperature_stream was not found. Was it deleted very recently?\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNABLE_TO_INFER_SCHEMA] Unable to infer schema for Parquet. It must be specified manually.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Now, create a table in the Spark catalog that points to your files.\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# The \"IF NOT EXISTS\" makes this cell safe to re-run.\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCreating table \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mschema_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m      9\u001b[39m \u001b[33;43m    CREATE TABLE IF NOT EXISTS \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mschema_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtable_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[33;43m    USING PARQUET\u001b[39;49m\n\u001b[32m     11\u001b[39m \u001b[33;43m    LOCATION \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtable_location\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     12\u001b[39m \u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTable created successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/aleksei-partanen-spark-structured-streaming/.venv/lib/python3.11/site-packages/pyspark/sql/session.py:1631\u001b[39m, in \u001b[36mSparkSession.sql\u001b[39m\u001b[34m(self, sqlQuery, args, **kwargs)\u001b[39m\n\u001b[32m   1627\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1628\u001b[39m         litArgs = \u001b[38;5;28mself\u001b[39m._jvm.PythonUtils.toArray(\n\u001b[32m   1629\u001b[39m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[32m   1630\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1631\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1632\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1633\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/aleksei-partanen-spark-structured-streaming/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/aleksei-partanen-spark-structured-streaming/.venv/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [UNABLE_TO_INFER_SCHEMA] Unable to infer schema for Parquet. It must be specified manually."
     ]
    }
   ],
   "source": [
    "# Give the stream a moment to write the first batch of data\n",
    "print(\"Waiting 15 seconds for the first micro-batch to complete...\")\n",
    "time.sleep(15)\n",
    "\n",
    "# Now, create a table in the Spark catalog that points to your files.\n",
    "# The \"IF NOT EXISTS\" makes this cell safe to re-run.\n",
    "print(f\"Creating table '{schema_name}.{table_name}'...\")\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {schema_name}.{table_name}\n",
    "    USING PARQUET\n",
    "    LOCATION '{table_location}'\n",
    "\"\"\")\n",
    "print(\"Table created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84afa5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sql(f'SELECT * FROM {schema_name}.{table_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b99c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aleksei-partanen-spark-structured-streaming",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
